lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)), xlim = c(0, 10), col = "red")
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
bin.size <- 0.5
h <- hist(means, plot=TRUE, col="blue", breaks=seq(0, ceiling(max(means)), bin.size))
plot(h, xlim = c(0, 10), col = "blue", main = "Histogram vs. PDF")
x = seq(0, 10, 0.01)
lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)), xlim = c(0, 10), col = "red")
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
bin.size <- 0.5
h <- hist(means, plot=TRUE, col="blue", breaks=seq(0, ceiling(max(means)), bin.size))
plot(h, xlim = c(0, 10), col = "blue", main = "Histogram vs. PDF suggested by CLT")
x = seq(0, 10, 0.01)
lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)), xlim = c(0, 10), col = "red")
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt
install.packages("pdflatex")
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
bin.size <- 0.5
h <- hist(means, plot=TRUE, col="blue", breaks=seq(0, ceiling(max(means)), bin.size))
plot(h, xlim = c(0, 10), col = "blue", main = "Histogram vs. PDF suggested by CLT")
x = seq(0, 10, 0.01)
lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)), xlim = c(0, 10), col = "red")
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt(n)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
bin.size <- 0.5
h <- hist(means, plot=TRUE, col="blue", breaks=seq(0, ceiling(max(means)), bin.size))
plot(h, xlim = c(0, 10), col = "blue", main = "Histogram vs. PDF suggested by CLT")
x = seq(0, 10, 0.01)
lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)), xlim = c(0, 10), col = "red")
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt(n)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
bin.size <- 0.5
h <- hist(means, plot=TRUE, col="blue", breaks=seq(0, ceiling(max(means)), bin.size))
plot(h, xlim = c(0, 10), col = "blue", main = "Histogram vs. PDF suggested by CLT")
x = seq(0, 10, 0.01)
lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)), xlim = c(0, 10), col = "red")
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt(n)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt(n)```
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt(n)```
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
n <- 40
num.iterations <- 1000
lambda <- .2
vals <- rexp(num.iterations, lambda)
hist(vals, col = "green")
means <- NULL
for (i in 1 : num.iterations) means <- c(means, mean(rexp(n, lambda)))
print(head(means, n = 10))
mean(means)
sd(means)
1 / lambda
1 / lambda / sqrt(n)
bin.size <- 0.5
h <- hist(means, plot=TRUE, col="blue", breaks=seq(0, ceiling(max(means)), bin.size))
plot(h, xlim = c(0, 10), col = "blue", main = "Histogram vs. PDF suggested by CLT")
x = seq(0, 10, 0.01)
lines(x, num.iterations * bin.size * dnorm(x, mean = 1/lambda, sd = 1 / lambda / sqrt(n)),
xlim = c(0, 10), col = "red")
df <- download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
df <- read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
head(df)
nrow(df[df$VAL >= 24, ])
df2 <- df[df$VAL >= 24, ]
df2$VAL
df2 <- df[df$VAL >= 24 & !is.na(df$VAL), ]
df2
df$FES
library(XLSX)
library(xlsx)
install.packages("XLSX")
install.packages("xlsx")
library(xlsx)
read.xlsx("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx")
read.xlsx("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", 1)
read.xlsx("C:/Users/Dominik/Documents", 1)
read.xlsx("C:/Users/Dominik/Documents")
read.xlsx("C:/Users/Dominik/Documents", 1)
read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1)
df <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1)
dat <- df[18:23, 7:15]
dat
sum(dat$Zip*dat$Ext,na.rm=T)
library(XML)
install.packages("XML")
library("XML")
xmlParseDoc("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml")
x <- xmlParseDoc("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml")
x
x <- xmlParseDoc("C:/Users/Dominik/Desktop/restaurant.xml")
x <- xmlParseDoc("C:/Users/Dominik/Desktop/restaurants.xml")
x
xmlToDataFrame(x)
df <- xmlToDataFrame(x)
df
head(df)
x
xmlToList(x)
df <- xmlTreeParse(x)
x <- xmlTreeParse("C:/Users/Dominik/Desktop/restaurants.xml")
x
xpathSApply(x, "//zipcode",xmlValue)
xpathApply(x, "//zipcode",xmlValue)
doc <- xmlRoot(x)
doc
xpathApply(doc, "//zipcode",xmlValue)
x <- xmlTreeParse("C:/Users/Dominik/Desktop/restaurants.xml", useInternalNodes = TRUE)
doc <- xmlRoot(x)
xpathApply(doc, "//zipcode",xmlValue)
xpathSApply(doc, "//zipcode",xmlValue)
z <- xpathSApply(doc, "//zipcode",xmlValue)
z
df <- data.frame(z = z)
tmp <- df[df$Z == 21231]
tmp
tmp <- df[df$Z == 21231, ]
tmp
tmp <- df[df$Z == "21231", ]
tmp
df$Z == "21231"
sum(df$Z == "21231")
df
df[df$Z == "21231", ]
d <- df[df$Z == "21231", ]
d
df$z
list(df$z == "21231")
sum(list(df$z == "21231"))
sum(df$z == "21231")
library(data.table)
install.packages("data.table")
df <- fread("C:/Users/Dominik/Desktop/pid.csv")
df <- f.read("C:/Users/Dominik/Desktop/pid.csv")
library("data.table")
df <- fread("C:/Users/Dominik/Desktop/pid.csv")
df
DT <- df
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
sapply(split(DT$pwgtp15,DT$SEX),mean)
?system.time
fun <- function(expr) { system.time(for i in 1:1000 { expr }}
fun <- function(expr) { system.time(for (i in 1:1000) { expr }}
fun <- function(expr) { system.time(for (i in 1:1000) { expr })}
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun <- function(expr) { system.time(for (i in 1:100000) { expr })}
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun <- function(expr) { system.time(for (i in 1:10000000) { expr })}
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(DT[,mean(pwgtp15),by=SEX])
fun(tapply(DT$pwgtp15,DT$SEX,mean))
fun <- function(expr) { system.time(for (i in 1:100000000) { expr })}
fun(tapply(DT$pwgtp15,DT$SEX,mean))
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
fun(mean(DT[DT$SEX==1,]$pwgtp15))
fun(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
fun(rowMeans(DT)[DT$SEX==1])
fun(rowMeans(DT)[DT$SEX==2])
df <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1)
df
df <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1, header = TRUE)
tmp <- df[18:23,7:15]
dat <- tmp
sum(dat$Zip*dat$Ext,na.rm=T)
dat
head(df)
df[1, ]
?read.xlsx
dat <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1, header = TRUE, rowIndex = 18:23, colIndex = 7:15)
sum(dat$Zip*dat$Ext,na.rm=T)
fun <- function(expr) { system.time(for (i in 1:200000000) { expr})}
library(data.table)
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(mean(DT[DT$SEX==1,]$pwgtp15))
fun(mean(DT[DT$SEX==2,]$pwgtp15))
fun(rowMeans(DT)[DT$SEX==1])
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(rowMeans(DT)[DT$SEX==1])
fun(DT[,mean(pwgtp15),by=SEX])
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(DT[,mean(pwgtp15),by=SEX])
if (outcome == "heart attack")
get.by.rank <- function(v, rank)
{
if (rank == "best") { v[1] }
else if (rank == "worst") { v[length(v)] }
else if (rank > length(v)) { NA }
else { v[rank] }
}
rankall <- function(outcome, rank = "best")
{
df <- read.table("outcome-of-care-measures.csv", sep = ",", colClasses = "character", header = TRUE)
df <- df[, c(2, 7, 11, 17, 23)]
colnames(df) = c("Hospital.Name", "State", "Heart.Attack", "Heart.Failure", "Pneumonia")
if (outcome == "heart attack")
{
df <- df[, c(1, 2, 3)]
}
else if (outcome == "heart failure")
{
df <- df[, c(1, 2, 4)]
}
else if (outcome == "pneumonia")
{
df <- df[, c(1, 2, 5)]
}
else
{
stop("invalid outcome")
}
colnames(df) <- c("Hospital.Name", "State", "Outcome")
df$Outcome <- as.numeric(df$Outcome)
df <- df[!is.na(df$Outcome), ]
df <- df[order(df$State, df$Outcome, df$Hospital.Name), ]
result <- NULL
states <- unique(df$State)
for (state in states)
{
result <- c(result, get.by.rank(df[df$State == state, 1], rank))
names(result)[length(result)] <- state
}
result <- data.frame(result, states)
colnames(result) <- c("hospital", "state")
invisible(result)
}
result
## Analysis file for the course project "Getting and Cleaning Data" on coursera
## Details on the processing steps can be found in README.txt
## Read files.
subject_test <- read.table("test/subject_test.txt", sep = "")
X_test <- read.table("test/X_test.txt", sep = "")
y_test <- read.table("test/y_test.txt", sep = "")
subject_train <- read.table("train/subject_train.txt", sep = "")
X_train <- read.table("train/X_train.txt", sep = "")
y_train <- read.table("train/y_train.txt", sep = "")
## Select features to extract based on the features.txt file.
## We are only interested in files about the mean and the standard deviation (std).
## Those always end with double parantheses ("()"). If only the word "mean" is contained,
## it will not be considered in this analysis.
## We extract the rows with "mean()" or "std()" in it using RegEx.
connection <- file("features.txt", open = "r")
features <- readLines(connection)
close.connection(connection)
selection.vector <- grep("mean\\(\\)|std\\(\\)", features, ignore.case = TRUE)
X_test <- X_test[, selection.vector]
X_train <- X_train[, selection.vector]
## Merge X, y and the subjects
df_test <- cbind(X_test, subject_test, y_test)
df_train <- cbind(X_train, subject_train, y_train)
## Concatenate the dataframes
df <- rbind(df_test, df_train)
## We are generating some useful descriptions for variable names now.
## First, we trim the leading row number and the following whitespace.
features <- gsub("[0-9]+ ", "", features[selection.vector])
## According to the documentation, the leading "t" abbreviates "time", "f" corresponds to "frequency",
## Acc" means "Acceleration", "Gyro" is "Gyroscope" and "Mag" stands for "magnitude".
features <- sub("^t", "time.", features)
features <- sub("^f", "frequency.", features)
features <- sub("Acc", ".acceleration.", features)
features <- sub("Gyro", ".gyroscope.", features)
features <- sub("Mag$|Mag-", "magnitude.", features)
features <- gsub("-", ".", features)
features <- gsub("\\.\\.", "\\.", features)
features <- tolower(features)
names(df) <- append(append(features, "subject"), "activity")
## Last, we will simply replace the activity ids in the column "activity" by the activity name.
connection <- file("activity_labels.txt", open = "r")
activity.names <- readLines(connection)
close.connection(connection)
mat <- matrix(unlist(strsplit(activity.names, split = " ")), nrow = 6, byrow = TRUE)
df$activity <- mat[df$activity, 2]
## For the second part of the assignment, we will use the "dplyr" package. We need to group the
## observations by subject and activity and calculate the mean for each single observation.
library(dplyr)
mean.df <- df %>% group_by(activity,subject) %>% summarise_all(mean)
names(mean.df)[3:length(names(mean.df))] <- paste0("mean.of.", names(mean.df)[3:length(names(mean.df))])
#write.table(mean.df, file = "tidy_means.csv", sep = ",", row.names = FALSE, col.names = FALSE)
---
title: "Human Activity Tracking Codebook"
author: "Dominik Sudwischer"
date: "17 September 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Extracting the Required Base Data
The original data set comprises 10299 obvervations collected from 30 volunteers that participated in a study capturing different movement measurements via the sensors of a Samsung Galacy S2 smartphone. According to the description of the original data set, the following facts hold true: The six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) had their corresponding motions tracked by an embedded accelerometer and a gyroscope. To be more precise, 3-axial linear acceleration and angular velocity have been captured at a constant rate of 50 measurements per seconds. The labeling of the data has been performend manually using recorded video files. In the original data, the 10299 observations have been split into two datasets with 70% ('X_train.txt') and 30% ('X_test.txt') of the records respectively. Originally, there are 561 features. The corresponding files 'y_train' and 'y_test' contain the id of the subject. The text file 'activity labels.txt' explain each activity id, a number between 1 and 6, in plain text.
These 5 files were concatenated in the following way: for $s \in \{\rm{"train"}, \rm{"test"}\}$, X_s and y_s have been combined using cbind. After that, both data frames have been concatenated using rbind. The activity ids have been replaced the the actual name strings for each activity that can be seen in 'activity labels.txt'.
## Included variables
After the initial transformations performed by the run_analysis script the data consists of a single large data frame including a total of 10299 records and 68 variables, 66 of which are extracted from the original dataset. The last two ones are the subject ID, an integer between 1 and 30, and the activity as a captial letter string. The 66 extracted variables are those that are mean and standard deviations of measurements. Please note that frequency means ('FreqMean()') are not included.
The resulting data set comprises means of each of those 66 variables, grouping the records by subject id and activity. The following variables are contained:
[1] "activity"
[2] "subject"
[3] "mean.of.time.body.acceleration.mean().x"
[4] "mean.of.time.body.acceleration.mean().y"
[5] "mean.of.time.body.acceleration.mean().z"
[6] "mean.of.time.body.acceleration.std().x"
[7] "mean.of.time.body.acceleration.std().y"
[8] "mean.of.time.body.acceleration.std().z"
[9] "mean.of.time.gravity.acceleration.mean().x"
[10] "mean.of.time.gravity.acceleration.mean().y"
[11] "mean.of.time.gravity.acceleration.mean().z"
[12] "mean.of.time.gravity.acceleration.std().x"
[13] "mean.of.time.gravity.acceleration.std().y"
[14] "mean.of.time.gravity.acceleration.std().z"
[15] "mean.of.time.body.acceleration.jerk.mean().x"
[16] "mean.of.time.body.acceleration.jerk.mean().y"
[17] "mean.of.time.body.acceleration.jerk.mean().z"
[18] "mean.of.time.body.acceleration.jerk.std().x"
[19] "mean.of.time.body.acceleration.jerk.std().y"
[20] "mean.of.time.body.acceleration.jerk.std().z"
[21] "mean.of.time.body.gyroscope.mean().x"
[22] "mean.of.time.body.gyroscope.mean().y"
[23] "mean.of.time.body.gyroscope.mean().z"
[24] "mean.of.time.body.gyroscope.std().x"
[25] "mean.of.time.body.gyroscope.std().y"
[26] "mean.of.time.body.gyroscope.std().z"
[27] "mean.of.time.body.gyroscope.jerk.mean().x"
[28] "mean.of.time.body.gyroscope.jerk.mean().y"
[29] "mean.of.time.body.gyroscope.jerk.mean().z"
[30] "mean.of.time.body.gyroscope.jerk.std().x"
[31] "mean.of.time.body.gyroscope.jerk.std().y"
[32] "mean.of.time.body.gyroscope.jerk.std().z"
[33] "mean.of.time.body.acceleration.magnitude.mean()"
[34] "mean.of.time.body.acceleration.magnitude.std()"
[35] "mean.of.time.gravity.acceleration.magnitude.mean()"
[36] "mean.of.time.gravity.acceleration.magnitude.std()"
[37] "mean.of.time.body.acceleration.jerkmagnitude.mean()"
[38] "mean.of.time.body.acceleration.jerkmagnitude.std()"
[39] "mean.of.time.body.gyroscope.magnitude.mean()"
[40] "mean.of.time.body.gyroscope.magnitude.std()"
[41] "mean.of.time.body.gyroscope.jerkmagnitude.mean()"
[42] "mean.of.time.body.gyroscope.jerkmagnitude.std()"
[43] "mean.of.frequency.body.acceleration.mean().x"
[44] "mean.of.frequency.body.acceleration.mean().y"
[45] "mean.of.frequency.body.acceleration.mean().z"
[46] "mean.of.frequency.body.acceleration.std().x"
[47] "mean.of.frequency.body.acceleration.std().y"
[48] "mean.of.frequency.body.acceleration.std().z"
[49] "mean.of.frequency.body.acceleration.jerk.mean().x"
[50] "mean.of.frequency.body.acceleration.jerk.mean().y"
[51] "mean.of.frequency.body.acceleration.jerk.mean().z"
[52] "mean.of.frequency.body.acceleration.jerk.std().x"
[53] "mean.of.frequency.body.acceleration.jerk.std().y"
[54] "mean.of.frequency.body.acceleration.jerk.std().z"
[55] "mean.of.frequency.body.gyroscope.mean().x"
[56] "mean.of.frequency.body.gyroscope.mean().y"
[57] "mean.of.frequency.body.gyroscope.mean().z"
[58] "mean.of.frequency.body.gyroscope.std().x"
[59] "mean.of.frequency.body.gyroscope.std().y"
[60] "mean.of.frequency.body.gyroscope.std().z"
[61] "mean.of.frequency.body.acceleration.magnitude.mean()"
[62] "mean.of.frequency.body.acceleration.magnitude.std()"
[63] "mean.of.frequency.bodybody.acceleration.jerkmagnitude.mean()"
[64] "mean.of.frequency.bodybody.acceleration.jerkmagnitude.std()"
[65] "mean.of.frequency.bodybody.gyroscope.magnitude.mean()"
[66] "mean.of.frequency.bodybody.gyroscope.magnitude.std()"
[67] "mean.of.frequency.bodybody.gyroscope.jerkmagnitude.mean()"
[68] "mean.of.frequency.bodybody.gyroscope.jerkmagnitude.std()"
The units are the same as in the original data. Please note that each feature has been normalized in the original data to be in [-1, 1]. The exact methods of generating the data, including filtering noise and applying Fourier transformations, can be looked up in the documentation of the original dataset. More detailed information can be found at http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones.
The names have been changed to be more readable by humans. In particular, leading 't' and 'f' have been renamed to 'time' and 'frequency', respectively. Also, 'Acc', 'Gyro' and 'Mag' have been renamed to 'acceleration', 'gyroscope' and 'magnitude'. All names have been transformed to lower case.
# Construction of Plot 4
# Load libraries
library(lubridate)
# Load data
Sys.setlocale("LC_TIME", "English")
unzip("exdata%2Fdata%2Fhousehold_power_consumption.zip")
df <- read.table("household_power_consumption.txt", sep = ";", header = TRUE,
colClasses = c("character", "character", "numeric", "numeric", "numeric", "numeric", "numeric",
"numeric", "numeric"), na.strings = c("?"))
# Process data
df <- df[as.character(df$Date) %in% c("1/2/2007", "2/2/2007"), ]
df$DT <- as.POSIXct(paste(df$Date, df$Time), format = "%d/%m/%Y %H:%M:%S")
#Plot 4
png(filename = "Plot4.png", width = 480, height = 480)
par(mfrow = c(2,2))
plot(df$DT, df$Global_active_power, type = "l", xlab = "", ylab = "Global Active Power")
plot(df$DT, df$Voltage, type = "l", xlab = "datetime", ylab = "Voltage")
plot(df$DT, df$Sub_metering_1, type = "l", col = "gray2", lwd = 2, xlab = "", ylab = "Energy sub metering")
lines(df$DT, df$Sub_metering_2, col = "red", lwd = 2)
lines(df$DT, df$Sub_metering_3, col = "blue", lwd = 2)
legend("topright", legend = c("sub_metering_1", "sub_metering_2", "sub_metering_3"),
col = c("gray2", "red", "blue"), lwd = 2)
plot(df$DT, df$Global_reactive_power, type = "l", xlab = "datetime", ylab = "Global_reactive_power", lwd = 1.5)
dev.off()
setwd("C:/Users/Dominik/MOOC/Coursera-Data-Science-Specialization/Exploratory Data Analysis/Assignment 1")
# Construction of Plot 4
# Load libraries
library(lubridate)
# Load data
Sys.setlocale("LC_TIME", "English")
unzip("exdata%2Fdata%2Fhousehold_power_consumption.zip")
df <- read.table("household_power_consumption.txt", sep = ";", header = TRUE,
colClasses = c("character", "character", "numeric", "numeric", "numeric", "numeric", "numeric",
"numeric", "numeric"), na.strings = c("?"))
# Process data
df <- df[as.character(df$Date) %in% c("1/2/2007", "2/2/2007"), ]
df$DT <- as.POSIXct(paste(df$Date, df$Time), format = "%d/%m/%Y %H:%M:%S")
#Plot 4
png(filename = "Plot4.png", width = 480, height = 480)
par(mfrow = c(2,2))
plot(df$DT, df$Global_active_power, type = "l", xlab = "", ylab = "Global Active Power")
plot(df$DT, df$Voltage, type = "l", xlab = "datetime", ylab = "Voltage")
plot(df$DT, df$Sub_metering_1, type = "l", col = "gray2", lwd = 2, xlab = "", ylab = "Energy sub metering")
lines(df$DT, df$Sub_metering_2, col = "red", lwd = 2)
lines(df$DT, df$Sub_metering_3, col = "blue", lwd = 2)
legend("topright", legend = c("sub_metering_1", "sub_metering_2", "sub_metering_3"),
col = c("gray2", "red", "blue"), lwd = 2)
plot(df$DT, df$Global_reactive_power, type = "l", xlab = "datetime", ylab = "Global_reactive_power", lwd = 1.5)
dev.off()
