df <- data.frame(z = z)
tmp <- df[df$Z == 21231]
tmp
tmp <- df[df$Z == 21231, ]
tmp
tmp <- df[df$Z == "21231", ]
tmp
df$Z == "21231"
sum(df$Z == "21231")
df
df[df$Z == "21231", ]
d <- df[df$Z == "21231", ]
d
df$z
list(df$z == "21231")
sum(list(df$z == "21231"))
sum(df$z == "21231")
library(data.table)
install.packages("data.table")
df <- fread("C:/Users/Dominik/Desktop/pid.csv")
df <- f.read("C:/Users/Dominik/Desktop/pid.csv")
library("data.table")
df <- fread("C:/Users/Dominik/Desktop/pid.csv")
df
DT <- df
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
sapply(split(DT$pwgtp15,DT$SEX),mean)
?system.time
fun <- function(expr) { system.time(for i in 1:1000 { expr }}
fun <- function(expr) { system.time(for (i in 1:1000) { expr }}
fun <- function(expr) { system.time(for (i in 1:1000) { expr })}
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun <- function(expr) { system.time(for (i in 1:100000) { expr })}
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun <- function(expr) { system.time(for (i in 1:10000000) { expr })}
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(DT[,mean(pwgtp15),by=SEX])
fun(tapply(DT$pwgtp15,DT$SEX,mean))
fun <- function(expr) { system.time(for (i in 1:100000000) { expr })}
fun(tapply(DT$pwgtp15,DT$SEX,mean))
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
fun(mean(DT[DT$SEX==1,]$pwgtp15))
fun(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
fun(rowMeans(DT)[DT$SEX==1])
fun(rowMeans(DT)[DT$SEX==2])
df <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1)
df
df <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1, header = TRUE)
tmp <- df[18:23,7:15]
dat <- tmp
sum(dat$Zip*dat$Ext,na.rm=T)
dat
head(df)
df[1, ]
?read.xlsx
dat <- read.xlsx("C:/Users/Dominik/Desktop/quiz.xlsx", 1, header = TRUE, rowIndex = 18:23, colIndex = 7:15)
sum(dat$Zip*dat$Ext,na.rm=T)
fun <- function(expr) { system.time(for (i in 1:200000000) { expr})}
library(data.table)
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(mean(DT[DT$SEX==1,]$pwgtp15))
fun(mean(DT[DT$SEX==2,]$pwgtp15))
fun(rowMeans(DT)[DT$SEX==1])
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(rowMeans(DT)[DT$SEX==1])
fun(DT[,mean(pwgtp15),by=SEX])
fun(mean(DT$pwgtp15,by=DT$SEX))
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(DT[,mean(pwgtp15),by=SEX])
fun(sapply(split(DT$pwgtp15,DT$SEX),mean))
fun(DT[,mean(pwgtp15),by=SEX])
if (outcome == "heart attack")
get.by.rank <- function(v, rank)
{
if (rank == "best") { v[1] }
else if (rank == "worst") { v[length(v)] }
else if (rank > length(v)) { NA }
else { v[rank] }
}
rankall <- function(outcome, rank = "best")
{
df <- read.table("outcome-of-care-measures.csv", sep = ",", colClasses = "character", header = TRUE)
df <- df[, c(2, 7, 11, 17, 23)]
colnames(df) = c("Hospital.Name", "State", "Heart.Attack", "Heart.Failure", "Pneumonia")
if (outcome == "heart attack")
{
df <- df[, c(1, 2, 3)]
}
else if (outcome == "heart failure")
{
df <- df[, c(1, 2, 4)]
}
else if (outcome == "pneumonia")
{
df <- df[, c(1, 2, 5)]
}
else
{
stop("invalid outcome")
}
colnames(df) <- c("Hospital.Name", "State", "Outcome")
df$Outcome <- as.numeric(df$Outcome)
df <- df[!is.na(df$Outcome), ]
df <- df[order(df$State, df$Outcome, df$Hospital.Name), ]
result <- NULL
states <- unique(df$State)
for (state in states)
{
result <- c(result, get.by.rank(df[df$State == state, 1], rank))
names(result)[length(result)] <- state
}
result <- data.frame(result, states)
colnames(result) <- c("hospital", "state")
invisible(result)
}
result
## Analysis file for the course project "Getting and Cleaning Data" on coursera
## Details on the processing steps can be found in README.txt
## Read files.
subject_test <- read.table("test/subject_test.txt", sep = "")
X_test <- read.table("test/X_test.txt", sep = "")
y_test <- read.table("test/y_test.txt", sep = "")
subject_train <- read.table("train/subject_train.txt", sep = "")
X_train <- read.table("train/X_train.txt", sep = "")
y_train <- read.table("train/y_train.txt", sep = "")
## Select features to extract based on the features.txt file.
## We are only interested in files about the mean and the standard deviation (std).
## Those always end with double parantheses ("()"). If only the word "mean" is contained,
## it will not be considered in this analysis.
## We extract the rows with "mean()" or "std()" in it using RegEx.
connection <- file("features.txt", open = "r")
features <- readLines(connection)
close.connection(connection)
selection.vector <- grep("mean\\(\\)|std\\(\\)", features, ignore.case = TRUE)
X_test <- X_test[, selection.vector]
X_train <- X_train[, selection.vector]
## Merge X, y and the subjects
df_test <- cbind(X_test, subject_test, y_test)
df_train <- cbind(X_train, subject_train, y_train)
## Concatenate the dataframes
df <- rbind(df_test, df_train)
## We are generating some useful descriptions for variable names now.
## First, we trim the leading row number and the following whitespace.
features <- gsub("[0-9]+ ", "", features[selection.vector])
## According to the documentation, the leading "t" abbreviates "time", "f" corresponds to "frequency",
## Acc" means "Acceleration", "Gyro" is "Gyroscope" and "Mag" stands for "magnitude".
features <- sub("^t", "time.", features)
features <- sub("^f", "frequency.", features)
features <- sub("Acc", ".acceleration.", features)
features <- sub("Gyro", ".gyroscope.", features)
features <- sub("Mag$|Mag-", "magnitude.", features)
features <- gsub("-", ".", features)
features <- gsub("\\.\\.", "\\.", features)
features <- tolower(features)
names(df) <- append(append(features, "subject"), "activity")
## Last, we will simply replace the activity ids in the column "activity" by the activity name.
connection <- file("activity_labels.txt", open = "r")
activity.names <- readLines(connection)
close.connection(connection)
mat <- matrix(unlist(strsplit(activity.names, split = " ")), nrow = 6, byrow = TRUE)
df$activity <- mat[df$activity, 2]
## For the second part of the assignment, we will use the "dplyr" package. We need to group the
## observations by subject and activity and calculate the mean for each single observation.
library(dplyr)
mean.df <- df %>% group_by(activity,subject) %>% summarise_all(mean)
names(mean.df)[3:length(names(mean.df))] <- paste0("mean.of.", names(mean.df)[3:length(names(mean.df))])
#write.table(mean.df, file = "tidy_means.csv", sep = ",", row.names = FALSE, col.names = FALSE)
---
title: "Human Activity Tracking Codebook"
author: "Dominik Sudwischer"
date: "17 September 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Extracting the Required Base Data
The original data set comprises 10299 obvervations collected from 30 volunteers that participated in a study capturing different movement measurements via the sensors of a Samsung Galacy S2 smartphone. According to the description of the original data set, the following facts hold true: The six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) had their corresponding motions tracked by an embedded accelerometer and a gyroscope. To be more precise, 3-axial linear acceleration and angular velocity have been captured at a constant rate of 50 measurements per seconds. The labeling of the data has been performend manually using recorded video files. In the original data, the 10299 observations have been split into two datasets with 70% ('X_train.txt') and 30% ('X_test.txt') of the records respectively. Originally, there are 561 features. The corresponding files 'y_train' and 'y_test' contain the id of the subject. The text file 'activity labels.txt' explain each activity id, a number between 1 and 6, in plain text.
These 5 files were concatenated in the following way: for $s \in \{\rm{"train"}, \rm{"test"}\}$, X_s and y_s have been combined using cbind. After that, both data frames have been concatenated using rbind. The activity ids have been replaced the the actual name strings for each activity that can be seen in 'activity labels.txt'.
## Included variables
After the initial transformations performed by the run_analysis script the data consists of a single large data frame including a total of 10299 records and 68 variables, 66 of which are extracted from the original dataset. The last two ones are the subject ID, an integer between 1 and 30, and the activity as a captial letter string. The 66 extracted variables are those that are mean and standard deviations of measurements. Please note that frequency means ('FreqMean()') are not included.
The resulting data set comprises means of each of those 66 variables, grouping the records by subject id and activity. The following variables are contained:
[1] "activity"
[2] "subject"
[3] "mean.of.time.body.acceleration.mean().x"
[4] "mean.of.time.body.acceleration.mean().y"
[5] "mean.of.time.body.acceleration.mean().z"
[6] "mean.of.time.body.acceleration.std().x"
[7] "mean.of.time.body.acceleration.std().y"
[8] "mean.of.time.body.acceleration.std().z"
[9] "mean.of.time.gravity.acceleration.mean().x"
[10] "mean.of.time.gravity.acceleration.mean().y"
[11] "mean.of.time.gravity.acceleration.mean().z"
[12] "mean.of.time.gravity.acceleration.std().x"
[13] "mean.of.time.gravity.acceleration.std().y"
[14] "mean.of.time.gravity.acceleration.std().z"
[15] "mean.of.time.body.acceleration.jerk.mean().x"
[16] "mean.of.time.body.acceleration.jerk.mean().y"
[17] "mean.of.time.body.acceleration.jerk.mean().z"
[18] "mean.of.time.body.acceleration.jerk.std().x"
[19] "mean.of.time.body.acceleration.jerk.std().y"
[20] "mean.of.time.body.acceleration.jerk.std().z"
[21] "mean.of.time.body.gyroscope.mean().x"
[22] "mean.of.time.body.gyroscope.mean().y"
[23] "mean.of.time.body.gyroscope.mean().z"
[24] "mean.of.time.body.gyroscope.std().x"
[25] "mean.of.time.body.gyroscope.std().y"
[26] "mean.of.time.body.gyroscope.std().z"
[27] "mean.of.time.body.gyroscope.jerk.mean().x"
[28] "mean.of.time.body.gyroscope.jerk.mean().y"
[29] "mean.of.time.body.gyroscope.jerk.mean().z"
[30] "mean.of.time.body.gyroscope.jerk.std().x"
[31] "mean.of.time.body.gyroscope.jerk.std().y"
[32] "mean.of.time.body.gyroscope.jerk.std().z"
[33] "mean.of.time.body.acceleration.magnitude.mean()"
[34] "mean.of.time.body.acceleration.magnitude.std()"
[35] "mean.of.time.gravity.acceleration.magnitude.mean()"
[36] "mean.of.time.gravity.acceleration.magnitude.std()"
[37] "mean.of.time.body.acceleration.jerkmagnitude.mean()"
[38] "mean.of.time.body.acceleration.jerkmagnitude.std()"
[39] "mean.of.time.body.gyroscope.magnitude.mean()"
[40] "mean.of.time.body.gyroscope.magnitude.std()"
[41] "mean.of.time.body.gyroscope.jerkmagnitude.mean()"
[42] "mean.of.time.body.gyroscope.jerkmagnitude.std()"
[43] "mean.of.frequency.body.acceleration.mean().x"
[44] "mean.of.frequency.body.acceleration.mean().y"
[45] "mean.of.frequency.body.acceleration.mean().z"
[46] "mean.of.frequency.body.acceleration.std().x"
[47] "mean.of.frequency.body.acceleration.std().y"
[48] "mean.of.frequency.body.acceleration.std().z"
[49] "mean.of.frequency.body.acceleration.jerk.mean().x"
[50] "mean.of.frequency.body.acceleration.jerk.mean().y"
[51] "mean.of.frequency.body.acceleration.jerk.mean().z"
[52] "mean.of.frequency.body.acceleration.jerk.std().x"
[53] "mean.of.frequency.body.acceleration.jerk.std().y"
[54] "mean.of.frequency.body.acceleration.jerk.std().z"
[55] "mean.of.frequency.body.gyroscope.mean().x"
[56] "mean.of.frequency.body.gyroscope.mean().y"
[57] "mean.of.frequency.body.gyroscope.mean().z"
[58] "mean.of.frequency.body.gyroscope.std().x"
[59] "mean.of.frequency.body.gyroscope.std().y"
[60] "mean.of.frequency.body.gyroscope.std().z"
[61] "mean.of.frequency.body.acceleration.magnitude.mean()"
[62] "mean.of.frequency.body.acceleration.magnitude.std()"
[63] "mean.of.frequency.bodybody.acceleration.jerkmagnitude.mean()"
[64] "mean.of.frequency.bodybody.acceleration.jerkmagnitude.std()"
[65] "mean.of.frequency.bodybody.gyroscope.magnitude.mean()"
[66] "mean.of.frequency.bodybody.gyroscope.magnitude.std()"
[67] "mean.of.frequency.bodybody.gyroscope.jerkmagnitude.mean()"
[68] "mean.of.frequency.bodybody.gyroscope.jerkmagnitude.std()"
The units are the same as in the original data. Please note that each feature has been normalized in the original data to be in [-1, 1]. The exact methods of generating the data, including filtering noise and applying Fourier transformations, can be looked up in the documentation of the original dataset. More detailed information can be found at http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones.
The names have been changed to be more readable by humans. In particular, leading 't' and 'f' have been renamed to 'time' and 'frequency', respectively. Also, 'Acc', 'Gyro' and 'Mag' have been renamed to 'acceleration', 'gyroscope' and 'magnitude'. All names have been transformed to lower case.
library(statsr)
library(dplyr)
library(ggplot2)
data(kobe_basket)
kobe_streak <- calc_streak(kobe_basket$shot)
ggplot(data = kobe_streak, aes(x = length)) +
geom_histogram(binwidth = 1)
coin_outcomes <- c("heads", "tails")
sample(coin_outcomes, size = 1, replace = TRUE)
?sample
sim_fair_coin <- sample(coin_outcomes, size = 100, replace = TRUE)
sim_fair_coin
table(sim_fair_coin)
sim_unfair_coin <- sample(coin_outcomes, size = 100, replace = TRUE,
prob = c(0.2, 0.8))
shot_outcomes <- c("H", "M")
sim_basket <- sample(shot_outcomes, size = 1, replace = TRUE)
# type your code for the Exercise here, and Knit
sim_basket <- sample(shot_outcomes, size = 133, prob = c(0.45, 0.55))
# type your code for the Exercise here, and Knit
sim_basket <- sample(shot_outcomes, size = 133, prob = c(0.45, 0.55), replace = TRUE)
# type your code for the Exercise here, and Knit
sim_streak <- calc_streak(sim_basket)
# type your code for the Exercise here, and Knit
ggplot(data = sim_streak, aes(x = sim_streak))+ geom_histogram(binwidth = 1)
# type your code for the Exercise here, and Knit
ggplot(data = sim_streak, aes(x = sim_streak))+ geom_histogram(binwidth = 1)
max(sim_streak)
?pbinom
pbinom(q = 0, size = 10, prob = 0.07, lower.tail = FALSE)
0.93^10
P(online | bad) = P(online AND bad) / P(bad) = 0.61
p_bad = 0.77 * 0.73 + 0.23 * 0.61
p_bad
0.61 / 0.7024
P(online | bad) = P(online and bad) / P(bad) = P(bad | online) * P(online) / b(bad)
0.61 * 0.23 / 0.7024
pbinom(q = 49, size = 160, prob = 0.28, lower.tail = FALSE)
pnorm(q = 50, mean = 160 * 0.28, sd = sqrt(160 * 0.28 * (1 - 160 * 0.28)), lower.tail = FALSE)
pnorm(q = 50, mean = 160 * 0.28, sd = sqrt(160 * 0.28 * (1 -  0.28)), lower.tail = FALSE)
# Plot 6
# Load packages
library(dplyr)
# Load data
unzip("exdata%2Fdata%2FNEI_data.zip")
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
NEI$type <- as.factor(NEI$type)
SCC$EI.Sector <- tolower(SCC$EI.Sector)
# Filter and aggregate data
# First, we filter for Baltimore City and Los Angeles and then look for emission sources in SCC with "vehicle"
# in it and finally merge (inner join) the filtered NEI with the filtered SCC to only keep relevant entries
NEI <- NEI[NEI$fips %in% c("24510", "06037"), ]
NEI$fips <- as.factor(NEI$fips)
SCC <- SCC[grepl("vehicle", SCC$EI.Sector), ]
NEI <- merge(NEI, SCC, by.x = "SCC", by.y = "SCC")
total <- NEI %>% group_by(NEI$year, NEI$fips) %>% summarise(total_tons_pm25 = sum(Emissions))
names(total)[c(1, 2)] <- c("year", "city")
# Subset data by city
los_angeles <- subset(total, city == "06037")
baltimore <- subset(total, city == "24510")
# Create the plot
#png(file = "plot6.png")
plot(los_angeles$year, los_angeles$total_tons_pm25, type = "o",
main = "Total PM2.5 Emissions from Vehicles in Baltimore City vs. Los Angeles", xlab = "Year",
ylab = "PM2.5 Emissions (tons)", xaxt = "n", col = "red",
ylim = c(0, max(los_angeles$total_tons_pm25)))
lines(baltimore$year, baltimore$total_tons_pm25, col = "green")
axis(side = 1, at = total$year, labels = total$year)
#dev.off()
setwd("C:/Users/Dominik/MOOC/Coursera-Data-Science-Specialization/Exploratory Data Analysis/Assignment 2")
# Plot 6
# Load packages
library(dplyr)
# Load data
unzip("exdata%2Fdata%2FNEI_data.zip")
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
NEI$type <- as.factor(NEI$type)
SCC$EI.Sector <- tolower(SCC$EI.Sector)
# Filter and aggregate data
# First, we filter for Baltimore City and Los Angeles and then look for emission sources in SCC with "vehicle"
# in it and finally merge (inner join) the filtered NEI with the filtered SCC to only keep relevant entries
NEI <- NEI[NEI$fips %in% c("24510", "06037"), ]
NEI$fips <- as.factor(NEI$fips)
SCC <- SCC[grepl("vehicle", SCC$EI.Sector), ]
NEI <- merge(NEI, SCC, by.x = "SCC", by.y = "SCC")
total <- NEI %>% group_by(NEI$year, NEI$fips) %>% summarise(total_tons_pm25 = sum(Emissions))
names(total)[c(1, 2)] <- c("year", "city")
# Subset data by city
los_angeles <- subset(total, city == "06037")
baltimore <- subset(total, city == "24510")
# Create the plot
#png(file = "plot6.png")
plot(los_angeles$year, los_angeles$total_tons_pm25, type = "o",
main = "Total PM2.5 Emissions from Vehicles in Baltimore City vs. Los Angeles", xlab = "Year",
ylab = "PM2.5 Emissions (tons)", xaxt = "n", col = "red",
ylim = c(0, max(los_angeles$total_tons_pm25)))
lines(baltimore$year, baltimore$total_tons_pm25, col = "green")
axis(side = 1, at = total$year, labels = total$year)
#dev.off()
plot(los_angeles$year, los_angeles$total_tons_pm25, type = "l", col = los_angeles$city)
plot(total$year, total$total_tons_pm25, type = "l", col = total$city)
plot(total$year, total$total_tons_pm25, type = "l", col = total$city)
total$city
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_line()
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_line() + facet_grid(~city)
los_angeles
los_angeles <- los_angeles %>% mutate(pct_change = (total_tons_pm25 / los_angeles[1, 3] - 1) * 100)
baltimore <- baltimore %>% mutate(pct_change = (total_tons_pm25 / baltimore[1, 3] - 1) * 100)
los_angeles
los_angeles <- los_angeles %>% mutate(pct_change = (total_tons_pm25 / tmp - 1) * 100)
tmp <- los_angeles[1, 3]
los_angeles <- los_angeles %>% mutate(pct_change = (total_tons_pm25 / tmp - 1) * 100)
los_angeles
los_angeles$pct_change
los_angeles$pct_change <- (total_tons_pm25 / tmp - 1) * 100
los_angeles$pct_change <- (los_angeles$total_tons_pm25 / tmp - 1) * 100
los_angeles$total_tons_pm25
los_angeles$total_tons_pm25 / 10
los_angeles$total_tons_pm25 / tmp
tmp
tmp <- los_angeles$year[1]
los_angeles$pct_change <- (los_angeles$total_tons_pm25 / tmp - 1) * 100
los_angeles
tmp <- los_angeles$total_tons_pm25[1]
los_angeles$pct_change <- (los_angeles$total_tons_pm25 / tmp - 1) * 100
los_angeles
tmp <- total %>% group_by(city) %>% summarise(base = first(total_tons_pm25, order_by = year))
tmp
total
base_data <- total %>%
group_by(city) %>%
summarise(base = first(total_tons_pm25, order_by = year)) %>%
select(city, total_tons_pm25)
base_data <- total %>%
group_by(city) %>%
summarise(base = first(total_tons_pm25, order_by = year)) %>%
select(city, base)
base_data
total <- merge(total, base_data, by = city)
total <- merge(total, base_data, by = "city")
total
total <- total %>%
mutate(pct_change = 100 * (total_tons_pm25 / base -1 )) %>%
filter(year > 1999)
total
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar(fill = city)
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar(fill = "city")
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar()
ggplot(data = total, aes(x = year)) + geom_bar()
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_col()
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_col(show.legend = TRUE)
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar(stat = "identity")
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar(stat = "identity") + scale_fill_discrete(name = "City")
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar(stat = "identity") + scale_fill_discrete(name = "City", breaks = c(1, 2), labels = c("C1", "C2"))
ggplot(data = total, aes(x = year, y = total_tons_pm25)) + geom_bar(stat = "identity") + scale_fill_discrete(name = "City", breaks = c(1, 2), labels = c("C1", "C2"))
ggplot(data = total, aes(x = year, y = total_tons_pm25), fill = City) + geom_bar(stat = "identity") + scale_fill_discrete(name = "City", breaks = c(1, 2), labels = c("C1", "C2"))
ggplot(data = total, aes(x = year, y = total_tons_pm25), fill = factor(City) + geom_bar(stat = "identity") + scale_fill_discrete(name = "City", breaks = c(1, 2), labels = c("C1", "C2"))
d
ggplot(data = total, aes(x = year, y = total_tons_pm25), fill = factor(City)) + geom_bar(stat = "identity") + scale_fill_discrete(name = "City", breaks = c(1, 2), labels = c("C1", "C2"))
ggplot(data = total, aes(x = year, y = total_tons_pm25), fill = factor(City)) + geom_bar(stat = "identity")
ggplot(data = total, aes(x = year, y = total_tons_pm25), fill = factor(City)) + geom_col()
ggplot(data = total, aes(x = year, y = total_tons_pm25, fill = factor(City)) + geom_col()
d
ggplot(data = total, aes(x = year, y = total_tons_pm25, fill = factor(City))) + geom_col()
ggplot(data = total, aes(x = year, y = total_tons_pm25, fill = factor(city))) + geom_col()
ggplot(data = total, aes(x = year, y = pct_change, fill = factor(city))) + geom_col()
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city))) + geom_col()
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city))) + geom_col(position = "dodge")
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city))) +
geom_col(position = "dodge") +
ggtitle("Change of PM 2.5 Emission Compared to the Base Year 1999") +
ylab("Emission Change (%)") +
xlab("Year") +
theme(legend.text = "City")
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city))) +
geom_col(position = "dodge") +
ggtitle("Change of PM 2.5 Emission Compared to the Base Year 1999") +
ylab("Emission Change (%)") +
xlab("Year") +
theme(legend.text = "City") +
labs(x = "lol", fill = "City")
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city))) +
geom_col(position = "dodge") +
ggtitle("Change of PM 2.5 Emission Compared to the Base Year 1999") +
ylab("Emission Change (%)") +
xlab("Year") +
labs(x = "lol", fill = "City")
# Plot 6
# Load packages
library(dplyr)
# Load data
unzip("exdata%2Fdata%2FNEI_data.zip")
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
NEI$type <- as.factor(NEI$type)
SCC$EI.Sector <- tolower(SCC$EI.Sector)
# Filter and aggregate data
# First, we filter for Baltimore City and Los Angeles and then look for emission sources in SCC with "vehicle"
# in it and finally merge (inner join) the filtered NEI with the filtered SCC to only keep relevant entries
NEI <- NEI[NEI$fips %in% c("24510", "06037"), ]
NEI$fips <- as.factor(NEI$fips)
SCC <- SCC[grepl("vehicle", SCC$EI.Sector), ]
NEI <- merge(NEI, SCC, by.x = "SCC", by.y = "SCC")
total <- NEI %>% group_by(NEI$year, NEI$fips) %>% summarise(total_tons_pm25 = sum(Emissions))
names(total)[c(1, 2)] <- c("year", "city")
# Normalize data with respect to the base year 1999 and calculate percentage change
base_data <- total %>%
group_by(city) %>%
summarise(base = first(total_tons_pm25, order_by = year)) %>%
select(city, base)
total <- merge(total, base_data, by = "city")
total <- total %>%
mutate(pct_change = 100 * (total_tons_pm25 / base -1 )) %>%
filter(year > 1999) %>%
mutate(city_name = ifelse(city == "24510", "Baltimore", "Los Angeles"))
# Note: Since I am only allowed to include exactly one plot, I chose percentage change as measurement.
# Under normal circumstances, I would have added a second plot to the figure to also show absolute changes.
# Create the plot
#png(file = "plot6.png")
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city_name))) +
geom_col(position = "dodge") +
ggtitle("Change of PM 2.5 Emission Compared to the Base Year 1999") +
labs(x = "Year", y = "Emission Change (%)", fill = "City")
#dev.off()
png(file = "plot6.png")
ggplot(data = total, aes(x = factor(year), y = pct_change, fill = factor(city_name))) +
geom_col(position = "dodge") +
ggtitle("Change of PM 2.5 Emission Compared to the Base Year 1999") +
labs(x = "Year", y = "Emission Change (%)", fill = "City")
dev.off()
# Plot 2
# Load packages
library(dplyr)
# Load data
unzip("exdata%2Fdata%2FNEI_data.zip")
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
NEI$type <- as.factor(NEI$type)
# Filter and aggregate data
NEI <- NEI[NEI$fips == "24510", ]
total <- NEI %>% group_by(NEI$year) %>% summarise(total_tons_pm25 = sum(Emissions))
names(total)[1] <- "year"
# Create the plot
png(file = "plot2.png")
plot(total$year, total$total_tons_pm25, type = "o",
main = "Total PM2.5 Emissions in Baltimore City, Maryland", xlab = "Year",
ylab = "PM2.5 Emissions (tons)", xaxt = "n")
axis(side = 1, at = total$year, labels = total$year)
dev.off()
